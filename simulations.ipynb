{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * # custom packages\n",
    "import ipympl\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width: 90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width: 90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 5\n",
    "num_classes = 2  # the number of units in the output layer\n",
    "hidden_size = 8  # the number of units in the recurrent layer\n",
    "batch_size = 1  # batch size = # of samples to average when computing gradient\n",
    "num_layers = 1  # number of stacked RNN/LSTM layers\n",
    "eta = 0.005  # learning rate - note that the learning rate had to increase by a factor of 10\n",
    "epochs = 1000  # epochs = # of full pases through dataset\n",
    "num_networks = 10 # number of networks to average when calculating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function, optimizer, and schedule (for decaying learning rate)\n",
    "criterion = nn.CrossEntropyLoss()  # loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(num_networks, condition, network_type='recurrent', generate_new=True, generate_random=True, same_distractions=False, verbose=False):\n",
    "    seqlen1, seqlen2, seqlen3 = condition[0], condition[1], condition[2]\n",
    "    losses = []\n",
    "    mean_loss = np.array([])\n",
    "    seeds = []\n",
    "    if verbose:\n",
    "        print('\\nLosses for', network_type, 'network:\\n')\n",
    "    for i in range(num_networks):\n",
    "        seed = RecurrentXORNet(input_size, hidden_size, num_layers, num_classes, batch_size, random_h0=True).to(device)\n",
    "        if network_type == 'lstm':\n",
    "            seed = LSTMXORNet(input_size, hidden_size, num_layers, num_classes, batch_size, random_h0=True, random_c0=True).to(device)\n",
    "        if network_type == 'gru':\n",
    "            seed = GRUXORNet(input_size, hidden_size, num_layers, num_classes, batch_size, random_h0=True).to(device)\n",
    "        optimizer = optim.Adam(seed.parameters(), eta)  # tells optimizer to adjust all parameter weights with steps based on eta\n",
    "        sheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=False) # lowers lr if the loss global min doesn't decrease for 5 epochs\n",
    "        dataset, targets, sequence_length = generate_dataset(same_distractions, input_size, seqlen1, seqlen2, seqlen3, random=generate_random)\n",
    "        loss = train_network(seed, dataset, targets, sequence_length, input_size, batch_size, epochs, optimizer, criterion, sheduler, generate_new=generate_new, generate_random=generate_random, same_distractions=same_distractions, condition=condition, verbose=verbose)\n",
    "        if i == 0:\n",
    "            mean_loss = loss\n",
    "        else:\n",
    "            mean_loss = mean_loss + loss\n",
    "        seeds.append(seed)\n",
    "        losses.append(loss)\n",
    "    losses = np.array(losses)\n",
    "    mean_loss = mean_loss/num_networks\n",
    "    return mean_loss, losses, seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_middle = [0, 3, 0] # small train in the middle\n",
    "large_middle = [0, 6, 0] # large train in the middle\n",
    "# xlarge_middle = [0, 20, 0] # xlarge train in the middle\n",
    "# xxlarge_middle = [0, 100, 0] # xxlarge train in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_small_recurrent,losses_small_recurrent,_ = get_loss(num_networks, small_middle)\n",
    "mean_loss_large_recurrent,losses_large_recurrent,_ = get_loss(num_networks, large_middle)\n",
    "mean_loss_small_lstm,losses_small_lstm,_ = get_loss(num_networks, small_middle, network_type='lstm')\n",
    "mean_loss_large_lstm,losses_large_lstm,_ = get_loss(num_networks, large_middle, network_type='lstm')\n",
    "mean_loss_small_gru,losses_small_gru,_ = get_loss(num_networks, small_middle, network_type='gru')\n",
    "mean_loss_large_gru,losses_large_gru,_ = get_loss(num_networks, large_middle, network_type='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adde884594e4df0aca32333049345fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.title(\"Average Effect of Distraction Train Length on Network Loss for Different Networks\", fontsize=10)\n",
    "\n",
    "## Recurrent losses\n",
    "plot_individual_losses(losses_small_recurrent, color='lightcoral', linewidth=0.7)\n",
    "plot_individual_losses(losses_large_recurrent, color='coral', linewidth=0.7)\n",
    "\n",
    "## LSTM losses\n",
    "plot_individual_losses(losses_small_lstm, color='steelblue', linewidth=0.7)\n",
    "plot_individual_losses(losses_large_lstm, color='skyblue', linewidth=0.7)\n",
    "\n",
    "## GRU losses\n",
    "plot_individual_losses(losses_small_gru, color='magenta', linewidth=0.7)\n",
    "plot_individual_losses(losses_large_gru, color='violet', linewidth=0.7)\n",
    "\n",
    "## Mean losses\n",
    "plt.plot(mean_loss_small_recurrent, color='red', label=\"Small middle train (mean, n=\" + str(num_networks) + \") - Recurrent\", linewidth=2)\n",
    "plt.plot(mean_loss_large_recurrent, color='orangered', label=\"Large middle train (mean, n=\" + str(num_networks) + \") - Recurrent\", linewidth=2)\n",
    "plt.plot(mean_loss_small_lstm, color='dodgerblue', label=\"Small middle train (mean, n=\" + str(num_networks) + \") - LSTM\", linewidth=2)\n",
    "plt.plot(mean_loss_large_lstm, color='deepskyblue', label=\"Large middle train (mean, n=\" + str(num_networks) + \") - LSTM\", linewidth=2)\n",
    "plt.plot(mean_loss_small_gru, color='darkmagenta', label=\"Small middle train (mean, n=\" + str(num_networks) + \") - GRU\", linewidth=2)\n",
    "plt.plot(mean_loss_large_gru, color='darkviolet', label=\"Large middle train (mean, n=\" + str(num_networks) + \") - GRU\", linewidth=2)\n",
    "\n",
    "\n",
    "# legend and show plot\n",
    "plt.legend(fontsize=8) # by default, the legend ignores all elements without a label attribute set.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
